{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras import initializers, layers, callbacks\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.45\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCLASSES = 10\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=NCLASSES)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=NCLASSES)\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Length(layers.Layer):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "def squash(inputs):\n",
    "    s_squared_norm = K.sum(K.square(inputs), axis=-1, keepdims=True)\n",
    "    scale = (s_squared_norm / (1 + s_squared_norm))\n",
    "    return scale * inputs / K.sqrt(s_squared_norm)    \n",
    "def margin_loss(y_true, y_pred):\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) +  0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "    return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capsule network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### capsule layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitCaps(layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 name='digitcaps'):\n",
    "        super(DigitCaps, self).__init__(name=name) # s√≥  pra  colocar nome nesta merda\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_vector = dim_vector\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "    def build(self, input_shape):\n",
    "        # shoud be [None,input_num_capsule,input_dim_vector]\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_vector = input_shape[2]\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W_ij = self.add_weight(shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n",
    "                                    initializer=self.kernel_initializer,name='W_ij')\n",
    "        self.b_i = self.add_weight(shape=[1, self.input_num_capsule, self.num_capsule, 1, 1],\n",
    "                                   initializer=self.bias_initializer,name='b_i', trainable=False)\n",
    "    def call(self, inputs, training=None):\n",
    "        # [None, input_num_capsule, input_dim_vector]\n",
    "        u_i = K.expand_dims(K.expand_dims(inputs, 2), 2)# [None, input_num_capsule, 1, 1, input_dim_vector]\n",
    "        u_i = K.tile(u_i, [1, 1, self.num_capsule, 1, 1]) # [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n",
    "        u_hat_ji = tf.scan(lambda ac, x: K.batch_dot(x, self.W_ij, [3, 2]), elems=u_i,\n",
    "                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n",
    "        for i in range(self.num_routing):\n",
    "            c_ij = tf.nn.softmax(self.b_i, dim=2)  \n",
    "            v_j = squash(K.sum(c_ij * u_hat_ji, 1, keepdims=True))\n",
    "            if i != self.num_routing - 1:\n",
    "                self.b_i += K.sum(u_hat_ji * v_j, -1, keepdims=True)\n",
    "        return K.reshape(v_j, [-1, self.num_capsule, self.dim_vector])\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_vector])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### primary caps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n",
    "    output = layers.Conv2D(filters=dim_vector*n_channels, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_vector])(output)\n",
    "    return layers.Lambda(squash)(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CapsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 784)\n",
      "Tensor(\"decoded/Reshape:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_74 (InputLayer)            (None, 28, 28, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, 20, 20, 256)   20992       input_74[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)               (None, 6, 6, 256)     5308672     conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "reshape_41 (Reshape)             (None, 1152, 8)       0           conv2d_41[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_49 (Lambda)               (None, 1152, 8)       0           reshape_41[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "digitcaps (DigitCaps)            (None, 10, 16)        1486080     lambda_49[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "input_75 (InputLayer)            (None, 10)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_50 (Lambda)               (None, 16)            0           digitcaps[0][0]                  \n",
      "                                                                   input_75[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_66 (Dense)                 (None, 512)           8704        lambda_50[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_67 (Dense)                 (None, 1024)          525312      dense_66[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_68 (Dense)                 (None, 784)           803600      dense_67[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "v_j_abs (Length)                 (None, 10)            0           digitcaps[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "decoded (Reshape)                (None, 28, 28, 1)     0           dense_68[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 8,153,360\n",
      "Trainable params: 8,141,840\n",
      "Non-trainable params: 11,520\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape=[28, 28, 1]\n",
    "num_routing=3\n",
    "x = layers.Input(shape=input_shape)\n",
    "# Conv1\n",
    "conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "# primary caps\n",
    "primarycaps = PrimaryCap(conv1, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "# digit caps\n",
    "digitcaps = DigitCaps(num_capsule=NCLASSES, dim_vector=16, num_routing=num_routing,name='digitcaps')(primarycaps)\n",
    "# predicted label\n",
    "#     v_j_abs = keras.layers.Lambda(Length)(digitcaps)\n",
    "v_j_abs = Length(name='v_j_abs')(digitcaps)\n",
    "## Reconstruction for regulariation\n",
    "y = layers.Input(shape=(NCLASSES,))\n",
    "\n",
    "def Mask(inputs): \n",
    "    return K.batch_dot(inputs[0], inputs[1], [1, 1])\n",
    "\n",
    "masked = layers.Lambda(Mask)([digitcaps, y])\n",
    "# masked = Mask()([digitcaps, y])\n",
    "l = layers.Dense(512, activation='relu')(masked)\n",
    "l = layers.Dense(1024, activation='relu')(l)\n",
    "l = layers.Dense(784, activation='sigmoid')(l)\n",
    "print(l.shape)\n",
    "decoded = layers.Reshape(target_shape=[28, 28, 1], name='decoded')(l)\n",
    "print(decoded)\n",
    "model = keras.models.Model([x, y], [v_j_abs, decoded])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, data, epocs):\n",
    "    (x_train, y_train), (x_test, y_test) = data\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=0.001, decay=0.0001),\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  loss_weights=[1., 28*28*0.0005],\n",
    "                  metrics={'v_j_abs': 'accuracy'})    \n",
    "    model.fit([x_train, y_train], [y_train, x_train], batch_size=256, epochs=epocs,\n",
    "              validation_data=[[x_test, y_test], [y_test, x_test]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 404s - loss: 0.1260 - v_j_abs_loss: 0.0997 - decoded_loss: 0.0669 - v_j_abs_acc: 0.9026 - val_loss: 0.0442 - val_v_j_abs_loss: 0.0250 - val_decoded_loss: 0.0489 - val_v_j_abs_acc: 0.9843\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 404s - loss: 0.0382 - v_j_abs_loss: 0.0209 - decoded_loss: 0.0441 - v_j_abs_acc: 0.9855 - val_loss: 0.0316 - val_v_j_abs_loss: 0.0164 - val_decoded_loss: 0.0390 - val_v_j_abs_acc: 0.9887\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 400s - loss: 0.0282 - v_j_abs_loss: 0.0140 - decoded_loss: 0.0363 - v_j_abs_acc: 0.9904 - val_loss: 0.0255 - val_v_j_abs_loss: 0.0124 - val_decoded_loss: 0.0334 - val_v_j_abs_acc: 0.9904\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 402s - loss: 0.0231 - v_j_abs_loss: 0.0107 - decoded_loss: 0.0317 - v_j_abs_acc: 0.9928 - val_loss: 0.0223 - val_v_j_abs_loss: 0.0108 - val_decoded_loss: 0.0293 - val_v_j_abs_acc: 0.9929\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 402s - loss: 0.0196 - v_j_abs_loss: 0.0085 - decoded_loss: 0.0283 - v_j_abs_acc: 0.9947 - val_loss: 0.0208 - val_v_j_abs_loss: 0.0104 - val_decoded_loss: 0.0264 - val_v_j_abs_acc: 0.9918\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 401s - loss: 0.0171 - v_j_abs_loss: 0.0070 - decoded_loss: 0.0257 - v_j_abs_acc: 0.9957 - val_loss: 0.0186 - val_v_j_abs_loss: 0.0089 - val_decoded_loss: 0.0247 - val_v_j_abs_acc: 0.9939\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 403s - loss: 0.0149 - v_j_abs_loss: 0.0055 - decoded_loss: 0.0238 - v_j_abs_acc: 0.9969 - val_loss: 0.0175 - val_v_j_abs_loss: 0.0085 - val_decoded_loss: 0.0228 - val_v_j_abs_acc: 0.9929\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 400s - loss: 0.0132 - v_j_abs_loss: 0.0044 - decoded_loss: 0.0224 - v_j_abs_acc: 0.9977 - val_loss: 0.0164 - val_v_j_abs_loss: 0.0079 - val_decoded_loss: 0.0217 - val_v_j_abs_acc: 0.9933\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 400s - loss: 0.0118 - v_j_abs_loss: 0.0035 - decoded_loss: 0.0211 - v_j_abs_acc: 0.9983 - val_loss: 0.0153 - val_v_j_abs_loss: 0.0073 - val_decoded_loss: 0.0205 - val_v_j_abs_acc: 0.9935\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 399s - loss: 0.0107 - v_j_abs_loss: 0.0028 - decoded_loss: 0.0201 - v_j_abs_acc: 0.9989 - val_loss: 0.0158 - val_v_j_abs_loss: 0.0083 - val_decoded_loss: 0.0193 - val_v_j_abs_acc: 0.9928\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 400s - loss: 0.0097 - v_j_abs_loss: 0.0022 - decoded_loss: 0.0191 - v_j_abs_acc: 0.9992 - val_loss: 0.0145 - val_v_j_abs_loss: 0.0072 - val_decoded_loss: 0.0185 - val_v_j_abs_acc: 0.9942\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 404s - loss: 0.0090 - v_j_abs_loss: 0.0018 - decoded_loss: 0.0182 - v_j_abs_acc: 0.9994 - val_loss: 0.0144 - val_v_j_abs_loss: 0.0074 - val_decoded_loss: 0.0178 - val_v_j_abs_acc: 0.9932\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 402s - loss: 0.0083 - v_j_abs_loss: 0.0014 - decoded_loss: 0.0174 - v_j_abs_acc: 0.9995 - val_loss: 0.0131 - val_v_j_abs_loss: 0.0065 - val_decoded_loss: 0.0169 - val_v_j_abs_acc: 0.9943\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 404s - loss: 0.0074 - v_j_abs_loss: 9.0367e-04 - decoded_loss: 0.0166 - v_j_abs_acc: 0.9997 - val_loss: 0.0131 - val_v_j_abs_loss: 0.0067 - val_decoded_loss: 0.0163 - val_v_j_abs_acc: 0.9934\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 406s - loss: 0.0069 - v_j_abs_loss: 7.0211e-04 - decoded_loss: 0.0159 - v_j_abs_acc: 0.9998 - val_loss: 0.0128 - val_v_j_abs_loss: 0.0067 - val_decoded_loss: 0.0156 - val_v_j_abs_acc: 0.9938\n",
      "Epoch 16/20\n",
      "41216/60000 [===================>..........] - ETA: 121s - loss: 0.0067 - v_j_abs_loss: 6.7578e-04 - decoded_loss: 0.0154 - v_j_abs_acc: 0.9998"
     ]
    }
   ],
   "source": [
    "train(model=model, data=((x_train, y_train), (x_test, y_test)), epocs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
